\documentclass[a4paper,twocolumn]{article} % Document type

\ifx\pdfoutput\undefined
    %Use old Latex if PDFLatex does not work
   \usepackage[dvips]{graphicx}% To get graphics working
   \DeclareGraphicsExtensions{.eps} % Encapsulated PostScript
 \else
    %Use PDFLatex
   \usepackage[pdftex]{graphicx}% To get graphics working
   \DeclareGraphicsExtensions{.pdf,.jpg,.png,.mps} % Portable Document Format, Joint Photographic Experts Group, Portable Network Graphics, MetaPost
   \pdfcompresslevel=9
\fi

\usepackage{amsmath,amssymb}   % Contains mathematical symbols
\usepackage[ansinew]{inputenc} % Input encoding, identical to Windows 1252
\usepackage[english]{babel}    % Language
%\usepackage[round,authoryear]{natbib}  %Nice author (year) citations
\usepackage[square,numbers]{natbib}     %Nice numbered citations
%\bibliographystyle{unsrtnat}           %Unsorted bibliography
\bibliographystyle{plainnat}            %Sorted bibliography

\addtolength{\topmargin}{-30mm}% Removes 30mm from the top margin
\addtolength{\textheight}{30mm}% Adds it to the text height


\begin{document}               % Begins the document

\title{Reproducing Sequential Attention for Feature Selection}
\author{Gary Wang, Hank Lai \\ N16132087, N16131722 \\ n16132087@gs.ncku.edu.tw, n16131722@gs.ncku.edu.tw} 
%\date{2010-10-10}             % If you want to set the date yourself.

\maketitle                     % Generates the title




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Instructions regarding the report
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Feature selection is essential for improving interpretability, generalization, and efficiency in high-dimensional learning tasks.
 However, As noted in a review by Nordling (2013, Chapter 5.2) have noted, it remains a challenging problem due to 
computational limits and sensitivity to noise. Classical methods such as LASSO and greedy search either fail to capture feature interactions or lack robustness.

Sequential Attention (SA)~\cite{yasuda2023} proposes a novel attention-based relaxation of greedy selection that models conditional importance adaptively. 
In this study, we aim to reproduce SA and evaluate whether it consistently outperforms classical methods 
like Sequential LASSO and Group LASSO.


\section{Problem Description}

Given a dataset $X \in \mathbb{R}^{n \times d}$ and labels $y \in \mathbb{R}^n$, the goal of feature selection is to find a subset $S \subseteq \{1, ..., d\}$ of $k$ features that maximize the predictive performance of a model $f(X_S)$.

Classical methods often ignore the marginal gain of features conditioned on already-selected features.
 SA addresses this by learning attention scores in a sequential manner. We evaluate whether SA more effectively 
identifies relevant subsets by comparing its classification accuracy and stability to that of Sequential LASSO and Group LASSO on benchmark datasets.

\section{Methods}
\subsection{Sequential Attention Algorithm}
% Description of algorithm, possibly Algorithm 1 from paper



\subsection{Experimental Setup}
% Dataset info, network architecture, optimizer, seeds, etc.

\section{Results}
\subsection{Dataset Overview}


\subsection{Feature Selection Accuracy}
% Accuracy table or plots for k=50 features


\subsection{Statistical Validation}
% t-test or p-value results table if needed

\section{Discussion}
% Analysis of differences, strengths, weaknesses, generalization, possible error sources

\section{Conclusion}
% Summary of what was reproduced and whether claims hold

\bibliographystyle{ieeetr}
\bibliography{references}


% Optionally include appendix
\clearpage
\appendix
\section*{Appendix}
\section{Hyperparameters}
% Learning rate, batch size, etc.

\section{Additional Figures}
% Supplementary plots

\end{document}      % End of the document
